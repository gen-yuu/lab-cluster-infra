---
# ==============================================================
# Role : gpu_node_common  ★ REV-3 (2025-06-24)
# Purpose : GPU ノードで NVIDIA Container Toolkit & Device-Plugin を
#           安定して構成する（libnvidia 1.17.8 系に統一）。
# ==============================================================

###############################
# 0. 変数
###############################
- name: "Set GPU-node facts"
  ansible.builtin.set_fact:
    nvidia_driver_lib_path: "/usr/lib/x86_64-linux-gnu"
    nvidia_ver: "1.17.8-1" # ⇦ 今後は vars ファイルで一元管理しても良い

###############################
# 1. リポジトリ登録（重複 list を整理）
###############################
- name: "Remove legacy NVIDIA lists (if any)"
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/apt/sources.list.d/libnvidia-container.list
    - /etc/apt/sources.list.d/nvidia-container-toolkit.list
  become: true

- name: "Add single stable/deb/amd64 repo list"
  ansible.builtin.copy:
    dest: /etc/apt/sources.list.d/nvidia-container-toolkit.list
    content: |
      deb [signed-by=/etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/amd64/ /
    owner: root
    group: root
    mode: "0644"
  become: true

- name: "Ensure GPG key present"
  ansible.builtin.shell: |
    mkdir -p /etc/apt/keyrings
    test -f /etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg || \
      curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
      gpg --dearmor -o /etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg
  args:
    executable: /bin/bash
  become: true

###############################
# 2. パッケージ整備
###############################
- name: "Purge legacy runtime & old libs"
  ansible.builtin.apt:
    name:
      - nvidia-container-runtime
      - nvidia-container-toolkit
      - nvidia-container-toolkit-base
      - libnvidia-container-tools
      - libnvidia-container1
    state: absent
    purge: yes
    allow_change_held_packages: yes
    force: yes
  become: true

- name: "Install NVIDIA toolkit stack {{ nvidia_ver }}"
  ansible.builtin.apt:
    name:
      - libnvidia-container1={{ nvidia_ver }}
      - libnvidia-container-tools={{ nvidia_ver }}
      - nvidia-container-toolkit-base={{ nvidia_ver }}
      - nvidia-container-toolkit={{ nvidia_ver }}
    state: present
    update_cache: yes
    allow_change_held_packages: yes
  become: true

###############################
# 3. containerd を NVIDIA ランタイムへ
###############################
- name: "Configure containerd via nvidia-ctk"
  ansible.builtin.command: nvidia-ctk runtime configure --runtime=containerd --set-as-default
  register: ctk_cfg
  changed_when: "'updated config' in ctk_cfg.stdout or 'INFO' in ctk_cfg.stdout"
  notify: Restart containerd and kubelet
  become: true

- name: 'Ensure default_runtime_name="nvidia" in /etc/containerd/config.toml'
  ansible.builtin.lineinfile:
    path: /etc/containerd/config.toml
    regexp: '^\s*default_runtime_name\s*=.*'
    line: '    default_runtime_name = "nvidia"'
    backrefs: yes
  notify: Restart containerd and kubelet
  become: true

###############################
# 4. ノードラベル
###############################
- name: "Label node as GPU-capable"
  ansible.builtin.command: kubectl label node {{ inventory_hostname }} nvidia.com/gpu.present=true --overwrite
  register: gpu_label
  changed_when: gpu_label.rc == 0 and 'not labeled' not in gpu_label.stdout
  failed_when: gpu_label.rc != 0 and 'AlreadyExists' not in gpu_label.stderr
  delegate_to: "{{ groups['k8s_control_plane'][0] }}"
  environment:
    KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
  become: false

###############################
# 5. Device Plugin (Helm)
###############################
- name: "Ensure NVIDIA Helm repo"
  kubernetes.core.helm_repository:
    name: nvdp
    repo_url: https://nvidia.github.io/k8s-device-plugin
    state: present
  delegate_to: "{{ groups['k8s_control_plane'][0] }}"
  environment:
    KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
  run_once: true

- name: "Deploy / upgrade NVIDIA Device Plugin v0.17.2"
  kubernetes.core.helm:
    name: nvdp-plugin
    chart_ref: nvdp/nvidia-device-plugin
    release_namespace: nvidia-device-plugin
    create_namespace: yes
    force: true
    wait: true
    values:
      image:
        repository: nvcr.io/nvidia/k8s-device-plugin
        tag: v0.17.2
      env: # ★ chart 直下
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64

      volumeMounts: # ★ chart 直下
        - name: host-lib
          mountPath: /usr/local/nvidia/lib64
          readOnly: true

      volumes: # ★ chart 直下
        - name: host-lib
          hostPath:
            path: /usr/lib/x86_64-linux-gnu
            type: Directory

      securityContext:
        privileged: true

      annotations:
        apparmor.security.beta.kubernetes.io/pod: unconfined
        apparmor.security.beta.kubernetes.io/nvidia-device-plugin-ctr: unconfined
        seccomp.security.alpha.kubernetes.io/pod: unconfined

    update_repo_cache: yes
  delegate_to: "{{ groups['k8s_control_plane'][0] }}"
  environment:
    KUBECONFIG: "/home/{{ ansible_user }}/.kube/config"
  run_once: true
  tags:
    - deploy_device_plugin
